{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wa1ZF-ytNhgb"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "# This is the size of our encoded representations\n",
        "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
        "\n",
        "# This is our input image\n",
        "input_img = keras.Input(shape=(784,))\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "encoded = layers.Dense(encoding_dim, activation='relu')(input_img)\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "# This model maps an input to its reconstruction\n",
        "autoencoder = keras.Model(input_img, decoded)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This model maps an input to its encoded representation\n",
        "encoder = keras.Model(input_img, encoded)"
      ],
      "metadata": {
        "id": "B0K62nGKNrK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is our encoded (32-dimensional) input\n",
        "encoded_input = keras.Input(shape=(encoding_dim,))\n",
        "# Retrieve the last layer of the autoencoder model\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "# Create the decoder model\n",
        "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))"
      ],
      "metadata": {
        "id": "-_RjUB7nNv6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n"
      ],
      "metadata": {
        "id": "Frd5LQ6KNzdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "(x_train, _), (x_test, _) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHf_CcjuN2Mn",
        "outputId": "fd01f44b-ff05-4e93-bca3-abf832626217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4Hq69i0N6Yd",
        "outputId": "04aa6362-bd56-4904-c28e-a61e09abfe6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=50,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI_dLeIWN88A",
        "outputId": "e2a69917-777b-4cb2-b159-f4f635ab9bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "235/235 [==============================] - 4s 13ms/step - loss: 0.2764 - val_loss: 0.1910\n",
            "Epoch 2/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.1719 - val_loss: 0.1550\n",
            "Epoch 3/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1452 - val_loss: 0.1341\n",
            "Epoch 4/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1291 - val_loss: 0.1220\n",
            "Epoch 5/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1192 - val_loss: 0.1139\n",
            "Epoch 6/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1119 - val_loss: 0.1079\n",
            "Epoch 7/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.1067 - val_loss: 0.1033\n",
            "Epoch 8/50\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1028 - val_loss: 0.1000\n",
            "Epoch 9/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0999 - val_loss: 0.0974\n",
            "Epoch 10/50\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.0979 - val_loss: 0.0958\n",
            "Epoch 11/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0965 - val_loss: 0.0947\n",
            "Epoch 12/50\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.0956 - val_loss: 0.0939\n",
            "Epoch 13/50\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.0950 - val_loss: 0.0935\n",
            "Epoch 14/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0946 - val_loss: 0.0931\n",
            "Epoch 15/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0943 - val_loss: 0.0929\n",
            "Epoch 16/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0941 - val_loss: 0.0927\n",
            "Epoch 17/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0939 - val_loss: 0.0927\n",
            "Epoch 18/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0938 - val_loss: 0.0924\n",
            "Epoch 19/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0937 - val_loss: 0.0923\n",
            "Epoch 20/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0936 - val_loss: 0.0922\n",
            "Epoch 21/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0935 - val_loss: 0.0922\n",
            "Epoch 22/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0934 - val_loss: 0.0921\n",
            "Epoch 23/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0934 - val_loss: 0.0921\n",
            "Epoch 24/50\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.0933 - val_loss: 0.0920\n",
            "Epoch 25/50\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.0933 - val_loss: 0.0920\n",
            "Epoch 26/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0932 - val_loss: 0.0919\n",
            "Epoch 27/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0932 - val_loss: 0.0919\n",
            "Epoch 28/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0931 - val_loss: 0.0919\n",
            "Epoch 29/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0931 - val_loss: 0.0918\n",
            "Epoch 30/50\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0931 - val_loss: 0.0918\n",
            "Epoch 31/50\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.0930 - val_loss: 0.0918\n",
            "Epoch 32/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0930 - val_loss: 0.0918\n",
            "Epoch 33/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0930 - val_loss: 0.0918\n",
            "Epoch 34/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0930 - val_loss: 0.0918\n",
            "Epoch 35/50\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0929 - val_loss: 0.0917\n",
            "Epoch 36/50\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.0929 - val_loss: 0.0919\n",
            "Epoch 37/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0929 - val_loss: 0.0917\n",
            "Epoch 38/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0929 - val_loss: 0.0919\n",
            "Epoch 39/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0929 - val_loss: 0.0917\n",
            "Epoch 40/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0928 - val_loss: 0.0917\n",
            "Epoch 41/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0928 - val_loss: 0.0916\n",
            "Epoch 42/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0928 - val_loss: 0.0916\n",
            "Epoch 43/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0928 - val_loss: 0.0916\n",
            "Epoch 44/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0928 - val_loss: 0.0917\n",
            "Epoch 45/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0928 - val_loss: 0.0916\n",
            "Epoch 46/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0927 - val_loss: 0.0916\n",
            "Epoch 47/50\n",
            "235/235 [==============================] - 2s 8ms/step - loss: 0.0927 - val_loss: 0.0916\n",
            "Epoch 48/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0927 - val_loss: 0.0916\n",
            "Epoch 49/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0927 - val_loss: 0.0916\n",
            "Epoch 50/50\n",
            "235/235 [==============================] - 2s 9ms/step - loss: 0.0927 - val_loss: 0.0916\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd09c927d00>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode and decode some digits\n",
        "# Note that we take them from the *test* set\n",
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQLmvNTcOt6A",
        "outputId": "e60e3690-afa6-4120-efd7-379de6b4d696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 0s 934us/step\n",
            "313/313 [==============================] - 0s 993us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Matplotlib (don't ask)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10  # How many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "oGf1IIagOAR8",
        "outputId": "f34afb79-e863-4bd7-90eb-2c999cc83610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x400 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import regularizers\n",
        "\n",
        "encoding_dim = 32\n",
        "\n",
        "input_img = keras.Input(shape=(784,))\n",
        "# Add a Dense layer with a L1 activity regularizer\n",
        "encoded = layers.Dense(encoding_dim, activation='relu',\n",
        "                activity_regularizer=regularizers.l1(10e-5))(input_img)\n",
        "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "autoencoder = keras.Model(input_img, decoded)"
      ],
      "metadata": {
        "id": "PnJJ_QeHO0TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_img = keras.Input(shape=(784,))\n",
        "encoded = layers.Dense(128, activation='relu')(input_img)\n",
        "encoded = layers.Dense(64, activation='relu')(encoded)\n",
        "encoded = layers.Dense(32, activation='relu')(encoded)\n",
        "\n",
        "decoded = layers.Dense(64, activation='relu')(encoded)\n",
        "decoded = layers.Dense(128, activation='relu')(decoded)\n",
        "decoded = layers.Dense(784, activation='sigmoid')(decoded)"
      ],
      "metadata": {
        "id": "UkFvPiTSO-35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder = keras.Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=100,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VnX84VdPDYq",
        "outputId": "fe4e7402-bc98-4a60-a1a2-8865ba2c6f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "235/235 [==============================] - 4s 14ms/step - loss: 0.2408 - val_loss: 0.1628\n",
            "Epoch 2/100\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.1469 - val_loss: 0.1341\n",
            "Epoch 3/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.1289 - val_loss: 0.1215\n",
            "Epoch 4/100\n",
            "235/235 [==============================] - 4s 15ms/step - loss: 0.1195 - val_loss: 0.1149\n",
            "Epoch 5/100\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 0.1141 - val_loss: 0.1104\n",
            "Epoch 6/100\n",
            "235/235 [==============================] - 4s 15ms/step - loss: 0.1103 - val_loss: 0.1077\n",
            "Epoch 7/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.1077 - val_loss: 0.1054\n",
            "Epoch 8/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.1056 - val_loss: 0.1035\n",
            "Epoch 9/100\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 0.1037 - val_loss: 0.1025\n",
            "Epoch 10/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.1019 - val_loss: 0.0999\n",
            "Epoch 11/100\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.1005 - val_loss: 0.0986\n",
            "Epoch 12/100\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.0992 - val_loss: 0.0972\n",
            "Epoch 13/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0980 - val_loss: 0.0966\n",
            "Epoch 14/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0971 - val_loss: 0.0955\n",
            "Epoch 15/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0961 - val_loss: 0.0946\n",
            "Epoch 16/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0953 - val_loss: 0.0942\n",
            "Epoch 17/100\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.0946 - val_loss: 0.0934\n",
            "Epoch 18/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0939 - val_loss: 0.0929\n",
            "Epoch 19/100\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.0934 - val_loss: 0.0921\n",
            "Epoch 20/100\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.0927 - val_loss: 0.0919\n",
            "Epoch 21/100\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0923 - val_loss: 0.0916\n",
            "Epoch 22/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0917 - val_loss: 0.0906\n",
            "Epoch 23/100\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.0913 - val_loss: 0.0905\n",
            "Epoch 24/100\n",
            "235/235 [==============================] - 7s 29ms/step - loss: 0.0909 - val_loss: 0.0899\n",
            "Epoch 25/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0906 - val_loss: 0.0898\n",
            "Epoch 26/100\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.0902 - val_loss: 0.0891\n",
            "Epoch 27/100\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0899 - val_loss: 0.0893\n",
            "Epoch 28/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0897 - val_loss: 0.0890\n",
            "Epoch 29/100\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 0.0894 - val_loss: 0.0889\n",
            "Epoch 30/100\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.0892 - val_loss: 0.0885\n",
            "Epoch 31/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0890 - val_loss: 0.0883\n",
            "Epoch 32/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0888 - val_loss: 0.0881\n",
            "Epoch 33/100\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.0885 - val_loss: 0.0878\n",
            "Epoch 34/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0884 - val_loss: 0.0879\n",
            "Epoch 35/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0882 - val_loss: 0.0880\n",
            "Epoch 36/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0880 - val_loss: 0.0873\n",
            "Epoch 37/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0879 - val_loss: 0.0871\n",
            "Epoch 38/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0878 - val_loss: 0.0874\n",
            "Epoch 39/100\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0877 - val_loss: 0.0872\n",
            "Epoch 40/100\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.0875 - val_loss: 0.0872\n",
            "Epoch 41/100\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.0874 - val_loss: 0.0868\n",
            "Epoch 42/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0873 - val_loss: 0.0865\n",
            "Epoch 43/100\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.0871 - val_loss: 0.0865\n",
            "Epoch 44/100\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.0870 - val_loss: 0.0867\n",
            "Epoch 45/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0869 - val_loss: 0.0863\n",
            "Epoch 46/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0868 - val_loss: 0.0862\n",
            "Epoch 47/100\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.0867 - val_loss: 0.0865\n",
            "Epoch 48/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0867 - val_loss: 0.0861\n",
            "Epoch 49/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0865 - val_loss: 0.0860\n",
            "Epoch 50/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0864 - val_loss: 0.0860\n",
            "Epoch 51/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0863 - val_loss: 0.0862\n",
            "Epoch 52/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0862 - val_loss: 0.0857\n",
            "Epoch 53/100\n",
            "235/235 [==============================] - 3s 15ms/step - loss: 0.0862 - val_loss: 0.0857\n",
            "Epoch 54/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0861 - val_loss: 0.0857\n",
            "Epoch 55/100\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.0860 - val_loss: 0.0855\n",
            "Epoch 56/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0859 - val_loss: 0.0855\n",
            "Epoch 57/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0859 - val_loss: 0.0853\n",
            "Epoch 58/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0858 - val_loss: 0.0854\n",
            "Epoch 59/100\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.0857 - val_loss: 0.0854\n",
            "Epoch 60/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0856 - val_loss: 0.0852\n",
            "Epoch 61/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0855 - val_loss: 0.0853\n",
            "Epoch 62/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0855 - val_loss: 0.0854\n",
            "Epoch 63/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0855 - val_loss: 0.0851\n",
            "Epoch 64/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0854 - val_loss: 0.0850\n",
            "Epoch 65/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0853 - val_loss: 0.0852\n",
            "Epoch 66/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0853 - val_loss: 0.0850\n",
            "Epoch 67/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0852 - val_loss: 0.0850\n",
            "Epoch 68/100\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.0852 - val_loss: 0.0850\n",
            "Epoch 69/100\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.0852 - val_loss: 0.0848\n",
            "Epoch 70/100\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.0851 - val_loss: 0.0851\n",
            "Epoch 71/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0850 - val_loss: 0.0847\n",
            "Epoch 72/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0850 - val_loss: 0.0849\n",
            "Epoch 73/100\n",
            "235/235 [==============================] - 4s 15ms/step - loss: 0.0850 - val_loss: 0.0846\n",
            "Epoch 74/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0849 - val_loss: 0.0848\n",
            "Epoch 75/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0849 - val_loss: 0.0847\n",
            "Epoch 76/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0849 - val_loss: 0.0846\n",
            "Epoch 77/100\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.0848 - val_loss: 0.0846\n",
            "Epoch 78/100\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.0848 - val_loss: 0.0847\n",
            "Epoch 79/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0848 - val_loss: 0.0847\n",
            "Epoch 80/100\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.0847 - val_loss: 0.0843\n",
            "Epoch 81/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0847 - val_loss: 0.0845\n",
            "Epoch 82/100\n",
            "235/235 [==============================] - 4s 15ms/step - loss: 0.0847 - val_loss: 0.0843\n",
            "Epoch 83/100\n",
            "235/235 [==============================] - 4s 15ms/step - loss: 0.0846 - val_loss: 0.0846\n",
            "Epoch 84/100\n",
            "235/235 [==============================] - 4s 15ms/step - loss: 0.0845 - val_loss: 0.0845\n",
            "Epoch 85/100\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.0845 - val_loss: 0.0844\n",
            "Epoch 86/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0845 - val_loss: 0.0843\n",
            "Epoch 87/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0845 - val_loss: 0.0844\n",
            "Epoch 88/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0845 - val_loss: 0.0842\n",
            "Epoch 89/100\n",
            "235/235 [==============================] - 4s 15ms/step - loss: 0.0844 - val_loss: 0.0841\n",
            "Epoch 90/100\n",
            "235/235 [==============================] - 3s 15ms/step - loss: 0.0844 - val_loss: 0.0841\n",
            "Epoch 91/100\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.0844 - val_loss: 0.0841\n",
            "Epoch 92/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0843 - val_loss: 0.0844\n",
            "Epoch 93/100\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.0843 - val_loss: 0.0840\n",
            "Epoch 94/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0843 - val_loss: 0.0839\n",
            "Epoch 95/100\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.0842 - val_loss: 0.0839\n",
            "Epoch 96/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0842 - val_loss: 0.0842\n",
            "Epoch 97/100\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.0842 - val_loss: 0.0838\n",
            "Epoch 98/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0842 - val_loss: 0.0839\n",
            "Epoch 99/100\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.0841 - val_loss: 0.0840\n",
            "Epoch 100/100\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.0841 - val_loss: 0.0839\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd09c9de290>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "input_img = keras.Input(shape=(28, 28, 1))\n",
        "\n",
        "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
        "\n",
        "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "x = layers.Conv2D(16, (3, 3), activation='relu')(x)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = keras.Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "mEgpxHIdPHKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))"
      ],
      "metadata": {
        "id": "miw6ojwVPJrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=50,\n",
        "                batch_size=128,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test),\n",
        "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LhlEdHUPNsu",
        "outputId": "d9b65de3-26d4-45b7-a02d-207241c3eaff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "469/469 [==============================] - 62s 129ms/step - loss: 0.2245 - val_loss: 0.1472\n",
            "Epoch 2/50\n",
            "469/469 [==============================] - 59s 125ms/step - loss: 0.1369 - val_loss: 0.1276\n",
            "Epoch 3/50\n",
            "469/469 [==============================] - 57s 123ms/step - loss: 0.1240 - val_loss: 0.1194\n",
            "Epoch 4/50\n",
            "469/469 [==============================] - 58s 124ms/step - loss: 0.1176 - val_loss: 0.1148\n",
            "Epoch 5/50\n",
            "469/469 [==============================] - 59s 126ms/step - loss: 0.1135 - val_loss: 0.1111\n",
            "Epoch 6/50\n",
            "469/469 [==============================] - 62s 131ms/step - loss: 0.1104 - val_loss: 0.1083\n",
            "Epoch 7/50\n",
            "469/469 [==============================] - 59s 127ms/step - loss: 0.1081 - val_loss: 0.1063\n",
            "Epoch 8/50\n",
            "469/469 [==============================] - 61s 129ms/step - loss: 0.1062 - val_loss: 0.1049\n",
            "Epoch 9/50\n",
            "469/469 [==============================] - 60s 128ms/step - loss: 0.1047 - val_loss: 0.1033\n",
            "Epoch 10/50\n",
            "469/469 [==============================] - 59s 126ms/step - loss: 0.1034 - val_loss: 0.1019\n",
            "Epoch 11/50\n",
            "469/469 [==============================] - 59s 126ms/step - loss: 0.1023 - val_loss: 0.1008\n",
            "Epoch 12/50\n",
            "469/469 [==============================] - 59s 125ms/step - loss: 0.1013 - val_loss: 0.0998\n",
            "Epoch 13/50\n",
            "469/469 [==============================] - 59s 126ms/step - loss: 0.1005 - val_loss: 0.0988\n",
            "Epoch 14/50\n",
            "469/469 [==============================] - 61s 131ms/step - loss: 0.0998 - val_loss: 0.0983\n",
            "Epoch 15/50\n",
            "469/469 [==============================] - 59s 126ms/step - loss: 0.0991 - val_loss: 0.0979\n",
            "Epoch 16/50\n",
            "469/469 [==============================] - 59s 125ms/step - loss: 0.0985 - val_loss: 0.0970\n",
            "Epoch 17/50\n",
            "469/469 [==============================] - 59s 125ms/step - loss: 0.0980 - val_loss: 0.0967\n",
            "Epoch 18/50\n",
            "469/469 [==============================] - 59s 126ms/step - loss: 0.0975 - val_loss: 0.0960\n",
            "Epoch 19/50\n",
            "469/469 [==============================] - 59s 126ms/step - loss: 0.0971 - val_loss: 0.0957\n",
            "Epoch 20/50\n",
            "469/469 [==============================] - 62s 131ms/step - loss: 0.0966 - val_loss: 0.0954\n",
            "Epoch 21/50\n",
            "469/469 [==============================] - 61s 131ms/step - loss: 0.0963 - val_loss: 0.0951\n",
            "Epoch 22/50\n",
            "469/469 [==============================] - 60s 128ms/step - loss: 0.0960 - val_loss: 0.0951\n",
            "Epoch 23/50\n",
            "469/469 [==============================] - 60s 128ms/step - loss: 0.0957 - val_loss: 0.0946\n",
            "Epoch 24/50\n",
            "469/469 [==============================] - 60s 128ms/step - loss: 0.0954 - val_loss: 0.0942\n",
            "Epoch 25/50\n",
            "469/469 [==============================] - 60s 127ms/step - loss: 0.0952 - val_loss: 0.0938\n",
            "Epoch 26/50\n",
            "469/469 [==============================] - 59s 127ms/step - loss: 0.0949 - val_loss: 0.0936\n",
            "Epoch 27/50\n",
            "469/469 [==============================] - 59s 127ms/step - loss: 0.0947 - val_loss: 0.0934\n",
            "Epoch 28/50\n",
            "469/469 [==============================] - 60s 127ms/step - loss: 0.0944 - val_loss: 0.0933\n",
            "Epoch 29/50\n",
            "469/469 [==============================] - 60s 127ms/step - loss: 0.0942 - val_loss: 0.0930\n",
            "Epoch 30/50\n",
            "469/469 [==============================] - 62s 131ms/step - loss: 0.0940 - val_loss: 0.0928\n",
            "Epoch 31/50\n",
            "469/469 [==============================] - 59s 126ms/step - loss: 0.0938 - val_loss: 0.0927\n",
            "Epoch 32/50\n",
            "469/469 [==============================] - 60s 127ms/step - loss: 0.0936 - val_loss: 0.0925\n",
            "Epoch 33/50\n",
            "469/469 [==============================] - 59s 127ms/step - loss: 0.0935 - val_loss: 0.0921\n",
            "Epoch 34/50\n",
            "469/469 [==============================] - 59s 126ms/step - loss: 0.0934 - val_loss: 0.0922\n",
            "Epoch 35/50\n",
            "469/469 [==============================] - 59s 126ms/step - loss: 0.0931 - val_loss: 0.0920\n",
            "Epoch 36/50\n",
            "469/469 [==============================] - 61s 130ms/step - loss: 0.0930 - val_loss: 0.0920\n",
            "Epoch 37/50\n",
            "469/469 [==============================] - 61s 129ms/step - loss: 0.0929 - val_loss: 0.0919\n",
            "Epoch 38/50\n",
            "469/469 [==============================] - 60s 127ms/step - loss: 0.0927 - val_loss: 0.0926\n",
            "Epoch 39/50\n",
            "469/469 [==============================] - 60s 127ms/step - loss: 0.0926 - val_loss: 0.0914\n",
            "Epoch 40/50\n",
            "469/469 [==============================] - 61s 130ms/step - loss: 0.0925 - val_loss: 0.0920\n",
            "Epoch 41/50\n",
            "469/469 [==============================] - 60s 128ms/step - loss: 0.0924 - val_loss: 0.0915\n",
            "Epoch 42/50\n",
            "469/469 [==============================] - 60s 129ms/step - loss: 0.0923 - val_loss: 0.0912\n",
            "Epoch 43/50\n",
            "469/469 [==============================] - 60s 128ms/step - loss: 0.0921 - val_loss: 0.0909\n",
            "Epoch 44/50\n",
            "469/469 [==============================] - 62s 132ms/step - loss: 0.0921 - val_loss: 0.0908\n",
            "Epoch 45/50\n",
            "469/469 [==============================] - 61s 130ms/step - loss: 0.0919 - val_loss: 0.0907\n",
            "Epoch 46/50\n",
            "469/469 [==============================] - 60s 129ms/step - loss: 0.0919 - val_loss: 0.0913\n",
            "Epoch 47/50\n",
            "469/469 [==============================] - 60s 129ms/step - loss: 0.0918 - val_loss: 0.0910\n",
            "Epoch 48/50\n",
            "469/469 [==============================] - 60s 128ms/step - loss: 0.0917 - val_loss: 0.0906\n",
            "Epoch 49/50\n",
            "469/469 [==============================] - 59s 127ms/step - loss: 0.0916 - val_loss: 0.0905\n",
            "Epoch 50/50\n",
            "469/469 [==============================] - 59s 127ms/step - loss: 0.0915 - val_loss: 0.0903\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd09ce7f490>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_imgs = autoencoder.predict(x_test)\n",
        "\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(1, n + 1):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yKLfTn7NPQXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = keras.Model(input_img, encoded)\n",
        "encoded_imgs = encoder.predict(x_test)\n",
        "\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 8))\n",
        "for i in range(1, n + 1):\n",
        "    ax = plt.subplot(1, n, i)\n",
        "    plt.imshow(encoded_imgs[i].reshape((4, 4 * 8)).T)\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VOn-_tvNPSnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
        "\n",
        "noise_factor = 0.5\n",
        "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "\n",
        "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
      ],
      "metadata": {
        "id": "y_PqLioOPVF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 2))\n",
        "for i in range(1, n + 1):\n",
        "    ax = plt.subplot(1, n, i)\n",
        "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cy05ZZy9PXd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_img = keras.Input(shape=(28, 28, 1))\n",
        "\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "# At this point the representation is (7, 7, 32)\n",
        "\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = keras.Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "KWS-iRfyPZ99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit(x_train_noisy, x_train,\n",
        "                epochs=100,\n",
        "                batch_size=128,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test_noisy, x_test),\n",
        "                callbacks=[TensorBoard(log_dir='/tmp/tb', histogram_freq=0, write_graph=False)])"
      ],
      "metadata": {
        "id": "LohOgz5VPckg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit(x_train_noisy, x_train,\n",
        "                epochs=100,\n",
        "                batch_size=128,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test_noisy, x_test),\n",
        "                callbacks=[TensorBoard(log_dir='/tmp/tb', histogram_freq=0, write_graph=False)])"
      ],
      "metadata": {
        "id": "iH2kTGHiPfhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps = ...  # Length of your sequences\n",
        "input_dim = ...\n",
        "latent_dim = ...\n",
        "\n",
        "inputs = keras.Input(shape=(timesteps, input_dim))\n",
        "encoded = layers.LSTM(latent_dim)(inputs)\n",
        "\n",
        "decoded = layers.RepeatVector(timesteps)(encoded)\n",
        "decoded = layers.LSTM(input_dim, return_sequences=True)(decoded)\n",
        "\n",
        "sequence_autoencoder = keras.Model(inputs, decoded)\n",
        "encoder = keras.Model(inputs, encoded)"
      ],
      "metadata": {
        "id": "dA5odOV3PiJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_dim = 28 * 28\n",
        "intermediate_dim = 64\n",
        "latent_dim = 2\n",
        "\n",
        "inputs = keras.Input(shape=(original_dim,))\n",
        "h = layers.Dense(intermediate_dim, activation='relu')(inputs)\n",
        "z_mean = layers.Dense(latent_dim)(h)\n",
        "z_log_sigma = layers.Dense(latent_dim)(h)"
      ],
      "metadata": {
        "id": "2UdD2EI2PkOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_sigma = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n",
        "                              mean=0., stddev=0.1)\n",
        "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
        "\n",
        "z = layers.Lambda(sampling)([z_mean, z_log_sigma])"
      ],
      "metadata": {
        "id": "BzG7jfpoPmQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create encoder\n",
        "encoder = keras.Model(inputs, [z_mean, z_log_sigma, z], name='encoder')\n",
        "\n",
        "# Create decoder\n",
        "latent_inputs = keras.Input(shape=(latent_dim,), name='z_sampling')\n",
        "x = layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
        "outputs = layers.Dense(original_dim, activation='sigmoid')(x)\n",
        "decoder = keras.Model(latent_inputs, outputs, name='decoder')\n",
        "\n",
        "# instantiate VAE model\n",
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = keras.Model(inputs, outputs, name='vae_mlp')"
      ],
      "metadata": {
        "id": "mRPH_CKkPoZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reconstruction_loss = keras.losses.binary_crossentropy(inputs, outputs)\n",
        "reconstruction_loss *= original_dim\n",
        "kl_loss = 1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer='adam')"
      ],
      "metadata": {
        "id": "6VEmxZpYPsCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "\n",
        "vae.fit(x_train, x_train,\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        validation_data=(x_test, x_test))"
      ],
      "metadata": {
        "id": "uvOJadHQPt-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-HBYPn0MPvsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a 2D manifold of the digits\n",
        "n = 15  # figure with 15x15 digits\n",
        "digit_size = 28\n",
        "figure = np.zeros((digit_size * n, digit_size * n))\n",
        "# We will sample n points within [-15, 15] standard deviations\n",
        "grid_x = np.linspace(-15, 15, n)\n",
        "grid_y = np.linspace(-15, 15, n)\n",
        "\n",
        "for i, yi in enumerate(grid_x):\n",
        "    for j, xi in enumerate(grid_y):\n",
        "        z_sample = np.array([[xi, yi]])\n",
        "        x_decoded = decoder.predict(z_sample)\n",
        "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "        figure[i * digit_size: (i + 1) * digit_size,\n",
        "               j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(figure)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E239pq9RPxtN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}