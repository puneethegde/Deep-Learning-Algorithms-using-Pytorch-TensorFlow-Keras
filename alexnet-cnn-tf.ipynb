{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n**The main content of this article will present how the AlexNet Convolutional Neural Network(CNN) architecture is implemented using TensorFlow and Keras.**\n\n\n**Here are some of the key learning objectives from this article:**\n\n1. Introduction to neural network implementation with Keras and TensorFlow\n2. Data preprocessing with TensorFlow\n3. Training visualization with TensorBoard\n4. Description of standard machine learning terms and terminologies\n5. AlexNet Implementation\n\nAlexNet CNN is probably one of the simplest methods to approach understanding deep learning concepts and techniques.\n\nAlexNet is not a complicated architecture when it is compared with some state of the art CNN architectures that have emerged in the more recent years.\n\nAlexNet is simple enough for beginners and intermediate deep learning practitioners to pick up some good practices on model implementation techniques.","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport os\nimport time\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to reference the class names of the images during the visualization stage, a python list containing the classes is initialized with the variable name CLASS_NAMES.","metadata":{}},{"cell_type":"code","source":"CLASS_NAMES= ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TensorFlow provides a suite of functions and operations that enables easy data manipulation and modification through a defined input pipeline.\n\nTo be able to access these methods and procedures, it is required that we transform our dataset into an efficient data representation TensorFlow is familiar with. This is achieved using the **tf.data.Dataset API**.\n\nMore specifically, **tf.data.Dataset.from_tensor_slices** method takes the train, test, and validation dataset partitions and returns a corresponding TensorFlow Dataset representation.","metadata":{}},{"cell_type":"code","source":"train_ds=tf.data.Dataset.from_tensor_slices((train_images,train_labels))\ntest_ds=tf.data.Dataset.from_tensor_slices((test_images,test_labels))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Usually, preprocessing is conducted to ensure the data utilized is within an appropriate format.\n\nFirst, let’s visualize the images within the CIFAR-10 dataset.\n\nThe CIFAR-10 images have small dimensions, which makes visualization of the actual pictures a bit difficult.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(30,30))\nfor i,(image,label) in enumerate(train_ds.take(20)):\n    #print(label)\n    ax=plt.subplot(5,5,i+1)\n    plt.imshow(image)\n    plt.title(CLASS_NAMES[label.numpy()[0]])\n    plt.axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We’ll create a function called process_images.\n\nThis function will perform all preprocessing work that we require for the data. This function is called further down the machine learning workflow.","metadata":{}},{"cell_type":"code","source":"def process_image(image,label):\n    image=tf.image.per_image_standardization(image)\n    image=tf.image.resize(image,(64,64))\n    \n    return image,label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pipeline","metadata":{}},{"cell_type":"markdown","source":"So far, we have obtained and partitioned the dataset and created a function to process the dataset. The next step is to build an input pipeline.\n\nAn input/data pipeline is described as a series of functions or methods that are called consecutively one after another. \n\nInput pipelines are a chain of functions that either act upon the data or enforces an operation on the data flowing through the pipeline.\n","metadata":{}},{"cell_type":"code","source":"train_ds_size=tf.data.experimental.cardinality(train_ds).numpy()\ntest_ds_size=tf.data.experimental.cardinality(test_ds).numpy()\nprint('Train size:',train_ds_size)\nprint('Test size:',test_ds_size)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### For our basic input/data pipeline, we will conduct three primary operations:\n \n#### 1. Preprocessing the data within the dataset\n#### 2. Shuffle the dataset\n#### 3. Batch data within the dataset","metadata":{}},{"cell_type":"code","source":"train_ds=(train_ds\n          .map(process_image)\n          .shuffle(buffer_size=train_ds_size)\n          .batch(batch_size=32,drop_remainder=True)\n         )\ntest_ds=(test_ds\n          .map(process_image)\n          .shuffle(buffer_size=test_ds_size)\n          .batch(batch_size=32,drop_remainder=True)\n         )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Implementation\n\nWithin this section, we will implement the AlexNet CNN architecture from scratch.\nThrough the utilization of Keras Sequential API, we can implement consecutive neural network layers within our models that are stacked against each other.\n\nHere are the types of layers the AlexNet CNN architecture is composed of, along with a brief description:\n## Convolutional layer:\nA convolution is a mathematical term that describes a dot product multiplication between two sets of elements. Within deep learning the convolution operation acts on the filters/kernels and image data array within the convolutional layer. Therefore a convolutional layer is simply a layer the houses the convolution operation that occurs between the filters and the images passed through a convolutional neural network.\n\n## Batch Normalisation layer:\n\nBatch Normalization is a technique that mitigates the effect of unstable gradients within a neural network through the introduction of an additional layer that performs operations on the inputs from the previous layer. The operations standardize and normalize the input values, after that the input values are transformed through scaling and shifting operations.\n\n## MaxPooling layer: \n\nMax pooling is a variant of sub-sampling where the maximum pixel value of pixels that fall within the receptive field of a unit within a sub-sampling layer is taken as the output. The max-pooling operation below has a window of 2x2 and slides across the input data, outputting an average of the pixels within the receptive field of the kernel.\n\n## Flatten layer: \n\nTakes an input shape and flattens the input image data into a one-dimensional array.\n\n## Dense Layer:\n\nA dense layer has an embedded number of arbitrary units/neurons within. Each neuron is a perceptron.\n\n\nThe code snippet represents the Keras implementation of the AlexNet CNN architecture.","metadata":{}},{"cell_type":"code","source":"model=keras.models.Sequential([\n    keras.layers.Conv2D(filters=128, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(64,64,3)),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=(2,2)),\n    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=(3,3)),\n    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=(2,2)),\n    keras.layers.Flatten(),\n    keras.layers.Dense(1024,activation='relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(1024,activation='relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(10,activation='softmax')  \n    \n    \n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and Results\n\nTo train the network, we have to compile it.\n\nThe compilation processes involve specifying the following items:\n\n## Loss function: \n\nA method that quantifies ‘how well’ a machine learning model performs. The quantification is an output(cost) based on a set of inputs, which are referred to as parameter values. The parameter values are used to estimate a prediction, and the ‘loss’ is the difference between the predictions and the actual values.\n\n## Optimization Algorithm: \n\nAn optimizer within a neural network is an algorithmic implementation that facilitates the process of gradient descent within a neural network by minimizing the loss values provided via the loss function. To reduce the loss, it is paramount the values of the weights within the network are selected appropriately.\n\n## Learning Rate:\n\nAn integral component of a neural network implementation detail as it’s a factor value that determines the level of updates that are made to the values of the weights of the network. Learning rate is a type of hyperparameter.","metadata":{}},{"cell_type":"markdown","source":"We can also provide a summary of the network to have more insight into the layer composition of the network by running the **model.summary()** function.","metadata":{}},{"cell_type":"code","source":"model.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer=tf.optimizers.SGD(lr=0.001),\n    metrics=['accuracy']    \n)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this point, we are ready to train the network.\n\nTraining the custom AlexNet network is very simple with the Keras module enabled through TensorFlow. We simply have to call the **fit()** method and pass relevant arguments.\n\n**Epoch**: This is a numeric value that indicates the number of time a network has been exposed to all the data points within a training dataset.","metadata":{}},{"cell_type":"code","source":"history=model.fit(\n    train_ds,\n    epochs=50,\n    validation_data=test_ds,\n    validation_freq=1\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.history.history.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will visualize the training over the different epochs .","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(2,1,figsize=(10,10)) \n\n#Assigning the first subplot to graph training loss and validation loss\nax[0].plot(model.history.history['loss'],color='b',label='Training Loss')\nax[0].plot(model.history.history['val_loss'],color='r',label='Validation Loss')\n\n#Plotting the training accuracy and validation accuracy\nax[1].plot(model.history.history['accuracy'],color='b',label='Training  Accuracy')\nax[1].plot(model.history.history['val_accuracy'],color='r',label='Validation Accuracy')\n\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy Score = ',np.max(history.history['val_accuracy']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# If you liked the kernel Please do Upvote and give your views in the comment section.","metadata":{}}]}